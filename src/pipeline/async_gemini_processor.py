#!/usr/bin/env python3
"""
LLKJJ ML Pipeline - Async Gemini-First Processor
=============================================

Performance-optimierte Version des GeminiDirectProcessor mit:
- Asynchroner Gemini API-Verarbeitung
- Batch-Processing fÃ¼r mehrere PDFs
- Rate-Limiting mit asyncio.Semaphore
- PDF-Hash-Caching gegen Duplikate

Autor: LLKJJ ML Pipeline Team
Version: 4.1.0 (Async Performance)
Datum: 18. August 2025
"""

import asyncio
import hashlib
import json
import logging
import sqlite3
import time
from datetime import datetime
from pathlib import Path
from typing import Any

from src.config import Config
from src.models.processing_result import ProcessingResult
from src.pipeline.gemini_first_processor import GeminiDirectProcessor

logger = logging.getLogger(__name__)


class AsyncGeminiDirectProcessor(GeminiDirectProcessor):
    """
    Async-optimierte Version des GeminiDirectProcessor.

    Performance-Features:
    - Async Gemini API-Calls mit Rate-Limiting
    - PDF-Hash-Caching in SQLite fÃ¼r Duplikatserkennung
    - Batch-Processing fÃ¼r multiple PDFs
    - Non-blocking I/O-Operationen
    """

    def __init__(self, config: Config | None = None):
        super().__init__(config)
        self.rate_limiter = asyncio.Semaphore(
            5
        )  # ðŸš€ ErhÃ¶ht: 3â†’5 concurrent Gemini calls
        self.cache_db_path = Path("data/cache/pdf_hash_cache.db")
        self._setup_cache_database()

        # ðŸ“Š MEMORY-OPTIMIERUNG: Batch-Processing Limits
        self.max_batch_size = 50  # Absolute Obergrenze fÃ¼r Memory-Schutz
        self.memory_check_interval = 10  # Alle 10 PDFs Memory-Check

        # ðŸ“„ SMART PDF-DETECTION: GrÃ¶ÃŸe-basierte Timeouts
        self.pdf_size_thresholds: dict[str, tuple[int, float]] = {
            "small": (0, 5 * 1024 * 1024),  # 0-5MB: Standard-Timeout
            "medium": (5 * 1024 * 1024, 20 * 1024 * 1024),  # 5-20MB: +50% Timeout
            "large": (20 * 1024 * 1024, 100 * 1024 * 1024),  # 20-100MB: +200% Timeout
            "huge": (100 * 1024 * 1024, float("inf")),  # >100MB: +500% Timeout
        }

        self.base_timeout = 30  # Sekunden fÃ¼r kleine PDFs

        logger.info(
            "âœ… AsyncGeminiDirectProcessor initialisiert (Rate-Limited + Memory-Optimized + Smart-Detection)"
        )

    def _setup_cache_database(self) -> None:
        """Setup SQLite database for PDF hash caching."""
        self.cache_db_path.parent.mkdir(parents=True, exist_ok=True)

        with sqlite3.connect(self.cache_db_path) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS pdf_cache (
                    pdf_hash TEXT PRIMARY KEY,
                    pdf_path TEXT NOT NULL,
                    processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    result_json TEXT NOT NULL,
                    confidence_score REAL NOT NULL,
                    extraction_quality TEXT NOT NULL
                )
            """
            )
            conn.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_processed_at
                ON pdf_cache(processed_at)
            """
            )

        logger.info("ðŸ”§ PDF-Hash-Cache-Database initialisiert: %s", self.cache_db_path)

    def _calculate_pdf_hash(self, pdf_path: Path) -> str:
        """Calculate SHA256 hash of PDF file for caching."""
        hasher = hashlib.sha256()
        with open(pdf_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hasher.update(chunk)
        return hasher.hexdigest()

    def _get_pdf_size_category(self, pdf_path: Path) -> tuple[str, int, float]:
        """
        ðŸš€ SMART PDF-DETECTION: Bestimme PDF-Kategorie basierend auf DateigrÃ¶ÃŸe.

        Returns:
            Tuple von (kategorie, timeout_sekunden, kompression_faktor)
        """
        try:
            file_size = pdf_path.stat().st_size

            for category, (min_size, max_size) in self.pdf_size_thresholds.items():
                if min_size <= file_size < max_size:
                    timeout_multiplier = {
                        "small": 1.0,  # Standard
                        "medium": 1.5,  # +50%
                        "large": 3.0,  # +200%
                        "huge": 6.0,  # +500%
                    }[category]

                    timeout = int(self.base_timeout * timeout_multiplier)
                    compression_factor = min(
                        1.0, 10 * 1024 * 1024 / file_size
                    )  # Komprimierung fÃ¼r >10MB

                    logger.info(
                        "ðŸ“„ PDF-Kategorie '%s': %s (%.2f MB) â†’ Timeout: %ds, Kompression: %.2f",
                        category,
                        pdf_path.name,
                        file_size / (1024 * 1024),
                        timeout,
                        compression_factor,
                    )

                    return category, timeout, compression_factor

            # Fallback
            return "small", self.base_timeout, 1.0

        except Exception as e:
            logger.warning(
                "âš ï¸ PDF-GrÃ¶ÃŸe-Detection fehlgeschlagen fÃ¼r %s: %s", pdf_path.name, e
            )
            return "small", self.base_timeout, 1.0

    def _check_memory_usage(self) -> dict[str, float]:
        """
        ðŸ“Š MEMORY-MONITORING: Ãœberwache aktuelle Memory-Usage fÃ¼r Batch-Processing.

        Returns:
            Dictionary mit Memory-Statistiken
        """
        import psutil

        process = psutil.Process()
        memory_info = process.memory_info()

        memory_stats: dict[str, float] = {
            "rss_mb": float(memory_info.rss / (1024 * 1024)),  # Resident Set Size
            "vms_mb": float(memory_info.vms / (1024 * 1024)),  # Virtual Memory Size
            "percent": float(process.memory_percent()),  # % of system memory
            "available_mb": float(psutil.virtual_memory().available / (1024 * 1024)),
        }

        logger.debug(
            "ðŸ’¾ Memory: RSS=%.1fMB, VMS=%.1fMB, Usage=%.1f%%, Available=%.1fGB",
            memory_stats["rss_mb"],
            memory_stats["vms_mb"],
            memory_stats["percent"],
            memory_stats["available_mb"] / 1024,
        )

        return memory_stats

    async def _check_cache(self, pdf_hash: str) -> dict[str, Any] | None:
        """Check if PDF result exists in cache."""

        def _db_query() -> tuple[str, float, float] | None:
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.execute(
                    """
                SELECT result_json, confidence_score, extraction_quality
                FROM pdf_cache WHERE pdf_hash = ?
            """,
                    (pdf_hash,),
                )
                row = cursor.fetchone()
                return row  # type: ignore[no-any-return]

        # Run database operation in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        row = await loop.run_in_executor(None, _db_query)

        if row:
            result_json, confidence_score, extraction_quality = row
            logger.info("ðŸŽ¯ Cache-Hit fÃ¼r PDF-Hash: %s", pdf_hash[:12])
            return {
                "result": json.loads(result_json),
                "confidence_score": confidence_score,
                "extraction_quality": extraction_quality,
                "from_cache": True,
            }

        return None

    async def _store_in_cache(
        self, pdf_hash: str, pdf_path: Path, result: ProcessingResult
    ) -> None:
        """Store processing result in cache."""

        def _db_insert() -> None:
            with sqlite3.connect(self.cache_db_path) as conn:
                conn.execute(
                    """
                    INSERT OR REPLACE INTO pdf_cache
                    (pdf_hash, pdf_path, result_json, confidence_score, extraction_quality)
                    VALUES (?, ?, ?, ?, ?)
                """,
                    (
                        pdf_hash,
                        str(pdf_path),
                        json.dumps(result.to_dict()),
                        result.confidence_score,
                        result.extraction_quality,
                    ),
                )  # Run database operation in thread pool

        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, _db_insert)

        logger.info("ðŸ’¾ PDF-Resultat gecacht fÃ¼r Hash: %s", pdf_hash[:12])

    async def process_pdf_async(self, pdf_path: str | Path) -> ProcessingResult:
        """
        Async-Version der PDF-Verarbeitung mit Caching.

        Performance-Optimierungen:
        1. PDF-Hash-Check fÃ¼r Duplikate (0ms wenn gecacht)
        2. Rate-Limited Gemini API-Calls
        3. Non-blocking I/O fÃ¼r File-Operations

        Args:
            pdf_path: Pfad zur PDF-Datei

        Returns:
            ProcessingResult mit optimierter Performance
        """
        start_time = time.time()
        pdf_path = Path(pdf_path)

        # 1. PDF-Hash fÃ¼r Cache-Check berechnen
        try:
            pdf_hash = await asyncio.get_event_loop().run_in_executor(
                None, self._calculate_pdf_hash, pdf_path
            )
            logger.info("ðŸ“Š PDF-Hash berechnet: %s (%s)", pdf_hash[:12], pdf_path.name)
        except Exception as e:
            logger.warning("âš ï¸ PDF-Hash-Berechnung fehlgeschlagen: %s", e)
            pdf_hash = None

        # 2. Cache-Check fÃ¼r sofortige Antwort
        if pdf_hash:
            cached_result = await self._check_cache(pdf_hash)
            if cached_result:
                cache_time_ms = int((time.time() - start_time) * 1000)
                logger.info(
                    "âš¡ Cache-Hit! PDF bereits verarbeitet in %dms", cache_time_ms
                )

                # ProcessingResult aus Cache rekonstruieren
                cached_data = cached_result["result"]
                return ProcessingResult(
                    pdf_path=str(pdf_path),
                    processing_timestamp=datetime.now().isoformat(),
                    processing_method="gemini_first",
                    structured_data=cached_data.get("structured_data", {}),
                    skr03_classifications=cached_data.get("skr03_classifications", []),
                    confidence_score=cached_result["confidence_score"],
                    extraction_quality=cached_result["extraction_quality"],
                    processing_time_ms=cache_time_ms,
                )

        # 3. Rate-Limited Gemini-Processing (Cache-Miss)
        async with self.rate_limiter:
            logger.info("ðŸš€ Processing PDF mit Rate-Limiting: %s", pdf_path.name)

            # Verwende synchronen Processor in Thread Pool
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, super().process_pdf_gemini_first, pdf_path
            )

        # 4. Ergebnis in Cache speichern
        if pdf_hash and result:
            try:
                await self._store_in_cache(pdf_hash, pdf_path, result)
            except Exception as e:
                logger.warning("âš ï¸ Cache-Speicherung fehlgeschlagen: %s", e)

        processing_time_ms = int((time.time() - start_time) * 1000)
        logger.info("âœ… Async PDF-Processing abgeschlossen in %dms", processing_time_ms)

        return result

    async def process_batch_async(
        self, pdf_paths: list[str | Path], max_concurrent: int = 3
    ) -> list[ProcessingResult | None]:
        """
        ðŸš€ ENHANCED Batch-Processing fÃ¼r mehrere PDFs mit Memory-Optimierung und Smart Detection.

        Performance-Features:
        - Memory-Monitoring alle 10 PDFs
        - Smart PDF-GrÃ¶ÃŸe-Detection fÃ¼r optimale Timeouts
        - Batch-GrÃ¶ÃŸe-Limiting bei >50 PDFs
        - Automatische Garbage Collection

        Args:
            pdf_paths: Liste der PDF-Pfade
            max_concurrent: Maximale gleichzeitige Verarbeitung

        Returns:
            Liste der ProcessingResults in derselben Reihenfolge
        """
        total_pdfs = len(pdf_paths)

        # ðŸ“Š MEMORY-SCHUTZ: Batch-GrÃ¶ÃŸe limitieren
        if total_pdfs > self.max_batch_size:
            logger.warning(
                "âš ï¸ Batch-GrÃ¶ÃŸe %d > Maximum %d - wird in Chunks aufgeteilt",
                total_pdfs,
                self.max_batch_size,
            )
            # Recursive batch processing in chunks
            all_results: list[ProcessingResult | None] = []
            for i in range(0, total_pdfs, self.max_batch_size):
                chunk = pdf_paths[i : i + self.max_batch_size]
                chunk_results = await self.process_batch_async(chunk, max_concurrent)
                all_results.extend(chunk_results)
            return all_results

        logger.info(
            "ðŸš€ Enhanced Batch-Processing startet: %d PDFs, max %d parallel (Memory-Optimized)",
            total_pdfs,
            max_concurrent,
        )

        start_time = time.time()
        initial_memory = self._check_memory_usage()

        # ðŸ“„ SMART PDF-ANALYSIS: Analysiere alle PDFs fÃ¼r optimale Strategie
        pdf_analysis: list[dict[str, Any]] = []
        for pdf_path in pdf_paths:
            category, timeout, compression = self._get_pdf_size_category(Path(pdf_path))
            pdf_analysis.append(
                {
                    "path": pdf_path,
                    "category": category,
                    "timeout": timeout,
                    "compression": compression,
                }
            )

        # Sortiere PDFs: GroÃŸe zuerst (bessere Memory-Verteilung)
        pdf_analysis.sort(key=lambda x: x["timeout"], reverse=True)
        logger.info(
            "ðŸ“Š PDF-Kategorien: %s",
            ", ".join(
                f"{info['category']}({Path(info['path']).name})"
                for info in pdf_analysis[:5]
            ),
        )

        # Semaphore fÃ¼r Batch-Limit (zusÃ¤tzlich zu API-Rate-Limit)
        batch_semaphore = asyncio.Semaphore(max_concurrent)
        processed_count = 0

        async def _process_with_enhanced_monitoring(
            pdf_info: dict[str, Any], index: int
        ) -> ProcessingResult | None:
            nonlocal processed_count

            async with batch_semaphore:
                try:
                    # ðŸ“Š MEMORY-CHECK alle 10 PDFs
                    if (
                        processed_count > 0
                        and processed_count % self.memory_check_interval == 0
                    ):
                        current_memory = self._check_memory_usage()
                        memory_growth = (
                            current_memory["rss_mb"] - initial_memory["rss_mb"]
                        )

                        if memory_growth > 500:  # >500MB Growth
                            logger.warning(
                                "âš ï¸ Memory-Growth: +%.1fMB nach %d PDFs - Garbage Collection",
                                memory_growth,
                                processed_count,
                            )
                            import gc

                            gc.collect()

                    result = await self.process_pdf_async(pdf_info["path"])
                    processed_count += 1

                    if processed_count % 10 == 0:
                        logger.info(
                            "ðŸ“ˆ Progress: %d/%d PDFs verarbeitet",
                            processed_count,
                            total_pdfs,
                        )

                    return result

                except Exception as e:
                    logger.error(
                        "âŒ PDF-Verarbeitung fehlgeschlagen: %s -> %s",
                        pdf_info["path"],
                        e,
                    )
                    return None

        # ðŸš€ Alle PDFs parallel verarbeiten mit Enhanced Monitoring
        tasks = [
            _process_with_enhanced_monitoring(pdf_info, i)
            for i, pdf_info in enumerate(pdf_analysis)
        ]
        results = await asyncio.gather(*tasks)

        # ðŸ“Š FINAL STATISTICS
        batch_time_ms = int((time.time() - start_time) * 1000)
        successful_count = sum(1 for r in results if r is not None)
        final_memory = self._check_memory_usage()
        memory_efficiency = final_memory["rss_mb"] / max(1, successful_count)

        logger.info(
            "âœ… Enhanced Batch-Processing abgeschlossen: %d/%d erfolgreich in %dms (%.1fMB/PDF avg)",
            successful_count,
            total_pdfs,
            batch_time_ms,
            memory_efficiency,
        )

        return results

    async def get_cache_statistics(self) -> dict[str, Any]:
        """
        ðŸŽ¯ QUICK WIN: Cache-Statistiken fÃ¼r Performance-Monitoring

        Returns:
            Dictionary mit Cache-Performance-Metriken
        """

        def _gather_stats() -> dict[str, Any]:
            with sqlite3.connect(self.cache_db_path) as conn:
                # Basis-Statistiken
                cursor = conn.execute("SELECT COUNT(*) FROM pdf_cache")
                total_entries = cursor.fetchone()[0]

                # Speicherplatz-Analyse
                cursor = conn.execute(
                    "SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()"
                )
                size_result = cursor.fetchone()
                db_size_bytes = size_result[0] if size_result else 0

                # Cache-Hit-Rate der letzten 24h
                cursor = conn.execute(
                    """
                    SELECT COUNT(*) FROM pdf_cache
                    WHERE processed_at > datetime('now', '-24 hours')
                """
                )
                recent_entries = cursor.fetchone()[0]

                # QualitÃ¤ts-Verteilung
                cursor = conn.execute(
                    """
                    SELECT extraction_quality, COUNT(*)
                    FROM pdf_cache
                    GROUP BY extraction_quality
                """
                )
                quality_dist = dict(cursor.fetchall())

                # Durchschnittliche Konfidenz
                cursor = conn.execute("SELECT AVG(confidence_score) FROM pdf_cache")
                avg_confidence = cursor.fetchone()[0] or 0.0

                return {
                    "total_cached_pdfs": total_entries,
                    "database_size_mb": round(db_size_bytes / (1024 * 1024), 2),
                    "cache_entries_24h": recent_entries,
                    "quality_distribution": quality_dist,
                    "average_confidence": round(avg_confidence, 3),
                    "estimated_savings_hours": round(
                        total_entries * 5.0 / 3600, 2
                    ),  # 5s pro PDF gespart
                }

        loop = asyncio.get_event_loop()
        stats = await loop.run_in_executor(None, _gather_stats)

        logger.info(
            "ðŸ“Š Cache-Statistiken: %d PDFs gecacht, %.2f MB, %.3f Ã˜ Konfidenz",
            stats["total_cached_pdfs"],
            stats["database_size_mb"],
            stats["average_confidence"],
        )

        return stats

    async def warm_cache_for_patterns(self, pattern_files: list[Path]) -> int:
        """
        ðŸŽ¯ QUICK WIN: Cache-Warming fÃ¼r hÃ¤ufige PDF-Patterns

        Analysiert hÃ¤ufige PDF-Muster und cached sie proaktiv.

        Args:
            pattern_files: Liste von PDF-Dateien mit typischen Mustern

        Returns:
            Anzahl neu gecachter PDFs
        """
        warmed_count = 0

        for pdf_path in pattern_files:
            if not pdf_path.exists():
                continue

            # PrÃ¼fe ob bereits gecacht
            try:
                pdf_hash = await asyncio.get_event_loop().run_in_executor(
                    None, self._calculate_pdf_hash, pdf_path
                )

                cached_result = await self._check_cache(pdf_hash)
                if cached_result:
                    logger.debug("âš¡ Pattern bereits gecacht: %s", pdf_path.name)
                    continue

                # Cache-Miss â†’ verarbeite und cache
                logger.info("ðŸ”¥ Cache-Warming fÃ¼r Pattern: %s", pdf_path.name)
                await self.process_pdf_async(pdf_path)
                warmed_count += 1

            except Exception as e:
                logger.warning(
                    "âš ï¸ Cache-Warming fehlgeschlagen fÃ¼r %s: %s", pdf_path.name, e
                )

        logger.info(
            "ðŸ”¥ Cache-Warming abgeschlossen: %d neue Patterns gecacht", warmed_count
        )
        return warmed_count

    async def optimize_cache_performance(self) -> dict[str, Any]:
        """
        ðŸŽ¯ QUICK WIN: Cache-Performance-Optimierung

        FÃ¼hrt Wartungsoperationen fÃ¼r bessere Cache-Performance durch.

        Returns:
            Optimierungsstatistiken
        """

        def _optimize_db() -> dict[str, Any]:
            with sqlite3.connect(self.cache_db_path) as conn:
                # VACUUM fÃ¼r Speicher-Optimierung
                conn.execute("VACUUM")

                # ANALYZE fÃ¼r Query-Optimierung
                conn.execute("ANALYZE")

                # ZusÃ¤tzliche Indizes fÃ¼r hÃ¤ufige Queries
                conn.execute(
                    """
                    CREATE INDEX IF NOT EXISTS idx_confidence_quality
                    ON pdf_cache(confidence_score, extraction_quality)
                """
                )

                # Cache-GrÃ¶ÃŸe nach Optimierung
                cursor = conn.execute(
                    "SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()"
                )
                optimized_size = cursor.fetchone()[0] if cursor.fetchone() else 0

                return {
                    "vacuum_completed": True,
                    "analyze_completed": True,
                    "new_indexes_created": 1,
                    "optimized_size_mb": round(optimized_size / (1024 * 1024), 2),
                }

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _optimize_db)

        logger.info(
            "âš¡ Cache-Optimierung abgeschlossen: %.2f MB nach VACUUM/ANALYZE",
            result["optimized_size_mb"],
        )

        return result

    async def clear_cache(self, older_than_days: int = 30) -> int:
        """
        Cache-Bereinigung fÃ¼r Ã¤ltere EintrÃ¤ge.

        Args:
            older_than_days: EintrÃ¤ge Ã¤lter als X Tage lÃ¶schen

        Returns:
            Anzahl gelÃ¶schter EintrÃ¤ge
        """

        def _db_cleanup() -> int:
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.execute(
                    f"""
                    DELETE FROM pdf_cache
                    WHERE processed_at < datetime('now', '-{older_than_days} days')
                """
                )
                return cursor.rowcount

        loop = asyncio.get_event_loop()
        deleted_count = await loop.run_in_executor(None, _db_cleanup)

        logger.info(
            "ðŸ§¹ Cache bereinigt: %d EintrÃ¤ge gelÃ¶scht (Ã¤lter als %d Tage)",
            deleted_count,
            older_than_days,
        )

        return deleted_count


# Convenience-Funktion fÃ¼r CLI-Integration
async def process_pdf_async(
    pdf_path: str, config: Config | None = None
) -> ProcessingResult:
    """Convenience function fÃ¼r async PDF-Processing."""
    processor = AsyncGeminiDirectProcessor(config)
    return await processor.process_pdf_async(pdf_path)


async def process_batch_async(
    pdf_paths: list[str | Path], config: Config | None = None, max_concurrent: int = 3
) -> list[ProcessingResult | None]:
    """Convenience function fÃ¼r async Batch-Processing."""
    processor = AsyncGeminiDirectProcessor(config)
    return await processor.process_batch_async(pdf_paths, max_concurrent)
